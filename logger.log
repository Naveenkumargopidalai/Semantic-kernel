INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=41, prompt_tokens=813, total_tokens=854)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=237, prompt_tokens=16, total_tokens=253)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=120, prompt_tokens=252, total_tokens=372)
INFO:semantic_kernel.planning.action_planner.action_planner:Finding the best function for achieving the goal: generate a java program on egg? 
INFO:semantic_kernel.planning.action_planner.action_planner:List of available functions:
// provided input generates a receipe for the input.
cooking.receipegenerator
Parameter ""input"": input.
// Summarize given text or any text document.
WriterPlugin.summarize
Parameter ""input"": Text to summarize.
// Summarize given text in three sentences .
WriterPlugin.ThreeSentencesummary
Parameter ""input"": input.
// Generic function, unknown purpose.
p_JeSfTmbCbLHSvZcU.f_TrzJxHKSvxEaIhMD
Parameter ""available_functions"": available_functions.
Parameter ""goal"": goal.
// Adds value to a value.
math.Add
Parameter ""input"": The value to add.
Parameter ""Amount"": Amount to add.
// Subtracts value to a value.
math.Subtract
Parameter ""input"": The value to subtract.
Parameter ""Amount"": Amount to subtract.
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=64, prompt_tokens=573, total_tokens=637)
INFO:semantic_kernel.planning.action_planner.action_planner:Plan generated by ActionPlanner:
{""plan"":{
""rationale"": ""the list contains a function that can generate a recipe, but it does not contain a function specifically for generating a java program"",
""function"": ""cooking.receipegenerator"",
""parameters"": {
""input"": ""egg""
}}}
#END-OF-PLAN
INFO:semantic_kernel.planning.action_planner.action_planner:Python dictionary of plan generated by ActionPlanner:
{'plan': {'rationale': 'the list contains a function that can generate a recipe, but it does not contain a function specifically for generating a java program', 'function': 'cooking.receipegenerator', 'parameters': {'input': 'egg'}}}
INFO:semantic_kernel.planning.action_planner.action_planner:ActionPlanner has picked cooking.receipegenerator. Reference to this function found in context: plugin_name='cooking' description='provided input generates a receipe for the input' name='receipegenerator' is_semantic=True stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x00000170D5521E40> parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)] delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7> function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x00000170D5521D00> plugins=KernelPluginCollection(plugins={'cooking': KernelPlugin(name='cooking', description=None, functions={'receipegenerator': KernelFunction(plugin_name='cooking', description='provided input generates a receipe for the input', name='receipegenerator', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x00000170D5521E40>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x00000170D5521D00>, plugins=KernelPluginCollection(plugins={...}), ai_service=OpenAIChatCompletion(ai_model_id='gpt-3.5-turbo-1106', client=<openai.AsyncOpenAI object at 0x00000170D4EFC230>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=1654, completion_tokens=462, total_tokens=2116), prompt_execution_settings=OpenAIChatPromptExecutionSettings(service_id=None, extension_data={}, ai_model_id='gpt-3.5-turbo-1106', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Provide a full receipe for egg recipe\n\n'}]), chat_prompt_template=None)}), 'WriterPlugin': KernelPlugin(name='WriterPlugin', description=None, functions={'summarize': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text or any text document', name='summarize', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x00000170D459B7E0>, parameters=[ParameterView(name='input', description='Text to summarize', default_value='', type_=None, required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x00000170D511B880>, plugins=KernelPluginCollection(plugins={...}), ai_service=OpenAIChatCompletion(ai_model_id='gpt-3.5-turbo-1106', client=<openai.AsyncOpenAI object at 0x00000170D4EFC230>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=1654, completion_tokens=462, total_tokens=2116), prompt_execution_settings=OpenAIChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 512, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id='gpt-3.5-turbo-1106', frequency_penalty=0.0, logit_bias={}, max_tokens=512, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\n\nSummarize this\nIngredients:\n- 4 eggs\n- 2 tablespoons of milk\n- Salt and pepper to taste\n- 1 tablespoon of butter or cooking oil\n- Optional toppings such as cheese, vegetables, or herbs\n\nInstructions:\n1. Crack the eggs into a bowl and add the milk, salt, and pepper. Whisk the mixture until well combined.\n2. Heat the butter or oil in a non-stick skillet over medium heat.\n3. Once the skillet is hot, pour the egg mixture into the skillet.\n4. Let the eggs cook undisturbed for a minute or two until the edges start to set.\n5. Using a spatula, gently push the cooked edges towards the center of the skillet, allowing the uncooked eggs to flow to the edges.\n6. Continue this process until the eggs are mostly set but still slightly runny on top.\n7. If using any toppings, sprinkle them over the eggs at this point.\n8. Carefully fold the eggs in half using the spatula and let them cook for another minute or until they are fully set.\n9. Slide the eggs onto a plate and serve hot.\n\nEnjoy your delicious and fluffy scrambled eggs!\n+++++'}]), chat_prompt_template=None), 'ThreeSentencesummary': KernelFunction(plugin_name='WriterPlugin', description='Summarize given text in three sentences ', name='ThreeSentencesummary', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x00000170D46BD300>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x00000170D46BCB80>, plugins=KernelPluginCollection(plugins={...}), ai_service=OpenAIChatCompletion(ai_model_id='gpt-3.5-turbo-1106', client=<openai.AsyncOpenAI object at 0x00000170D4EFC230>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=1654, completion_tokens=462, total_tokens=2116), prompt_execution_settings=OpenAIChatPromptExecutionSettings(service_id='default', extension_data={'max_tokens': 100, 'temperature': 0.0, 'top_p': 0.0, 'presence_penalty': 0.0, 'frequency_penalty': 0.0}, ai_model_id=None, frequency_penalty=0.0, logit_bias={}, max_tokens=100, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=0.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=None), chat_prompt_template=None)}), 'math': KernelPlugin(name='math', description=None, functions={'Add': KernelFunction(plugin_name='math', description='Adds value to a value', name='Add', is_semantic=False, stream_function=<bound method MathPlugin.add of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to add', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to add', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.add of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'Subtract': KernelFunction(plugin_name='math', description='Subtracts value to a value', name='Subtract', is_semantic=False, stream_function=<bound method MathPlugin.subtract of MathPlugin()>, parameters=[ParameterView(name='input', description='The value to subtract', default_value='', type_='string', required=False), ParameterView(name='Amount', description='Amount to subtract', default_value='', type_='number', required=True)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method MathPlugin.subtract of MathPlugin()>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)}), 'p_JeSfTmbCbLHSvZcU': KernelPlugin(name='p_JeSfTmbCbLHSvZcU', description=None, functions={'f_TrzJxHKSvxEaIhMD': KernelFunction(plugin_name='p_JeSfTmbCbLHSvZcU', description='Generic function, unknown purpose', name='f_TrzJxHKSvxEaIhMD', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x00000170D5523600>, parameters=[ParameterView(name='available_functions', description='', default_value='', type_='string', required=False), ParameterView(name='goal', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x00000170D55237E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=OpenAIChatCompletion(ai_model_id='gpt-3.5-turbo-1106', client=<openai.AsyncOpenAI object at 0x00000170D4EFC230>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=1654, completion_tokens=462, total_tokens=2116), prompt_execution_settings=OpenAIChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1000, 'temperature': 0.8}, ai_model_id='gpt-3.5-turbo-1106', frequency_penalty=0.0, logit_bias={}, max_tokens=1000, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.8, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': '\nYou are a planner for the Semantic Kernel.\nYour job is to create a properly formatted JSON plan step by step, to satisfy the goal given.\nCreate a list of subtasks based off the [GOAL] provided.\nEach subtask must be from within the [AVAILABLE FUNCTIONS] list. Do not use any functions that are not in the list.\nBase your decisions on which functions to use from the description and the name of the function.\nSometimes, a function may take arguments. Provide them if necessary.\nThe plan should be as short as possible.\nFor example:\n\n[AVAILABLE FUNCTIONS]\nEmailConnector.LookupContactEmail\ndescription: looks up the a contact and retrieves their email address\nargs:\n- name: the name to look up\n\nWriterPlugin.EmailTo\ndescription: email the input text to a recipient\nargs:\n- input: the text to email\n- recipient: the recipient\'s email address. Multiple addresses may be included if separated by \';\'.\n\nWriterPlugin.Translate\ndescription: translate the input to another language\nargs:\n- input: the text to translate\n- language: the language to translate to\n\nWriterPlugin.Summarize\ndescription: summarize input text\nargs:\n- input: the text to summarize\n\nFunPlugin.Joke\ndescription: Generate a funny joke\nargs:\n- input: the input to generate a joke about\n\n[GOAL]\n"Tell a joke about cars. Translate it to Spanish"\n\n[OUTPUT]\n    {\n        "input": "cars",\n        "subtasks": [\n            {"function": "FunPlugin.Joke"},\n            {"function": "WriterPlugin.Translate", "args": {"language": "Spanish"}}\n        ]\n    }\n\n[AVAILABLE FUNCTIONS]\nWriterPlugin.Brainstorm\ndescription: Brainstorm ideas\nargs:\n- input: the input to brainstorm about\n\nEdgarAllenPoePlugin.Poe\ndescription: Write in the style of author Edgar Allen Poe\nargs:\n- input: the input to write about\n\nWriterPlugin.EmailTo\ndescription: Write an email to a recipient\nargs:\n- input: the input to write about\n- recipient: the recipient\'s email address.\n\nWriterPlugin.Translate\ndescription: translate the input to another language\nargs:\n- input: the text to translate\n- language: the language to translate to\n\n[GOAL]\n"Tomorrow is Valentine\'s day. I need to come up with a few date ideas.\nShe likes Edgar Allen Poe so write using his style.\nE-mail these ideas to my significant other. Translate it to French."\n\n[OUTPUT]\n    {\n        "input": "Valentine\'s Day Date Ideas",\n        "subtasks": [\n            {"function": "WriterPlugin.Brainstorm"},\n            {"function": "EdgarAllenPoePlugin.Poe"},\n            {"function": "WriterPlugin.EmailTo", "args": {"recipient": "significant_other"}},\n            {"function": "WriterPlugin.Translate", "args": {"language": "French"}}\n        ]\n    }\n\n[AVAILABLE FUNCTIONS]\nmath.Add\ndescription: Adds value to a value\nargs:\n- input: The value to add\n- Amount: Amount to add\n\nmath.Subtract\ndescription: Subtracts value to a value\nargs:\n- input: The value to subtract\n- Amount: Amount to subtract\n\ncooking.receipegenerator\ndescription: provided input generates a receipe for the input\nargs:\n- input: \n\nWriterPlugin.summarize\ndescription: Summarize given text or any text document\nargs:\n- input: Text to summarize\n\nWriterPlugin.ThreeSentencesummary\ndescription: Summarize given text in three sentences \nargs:\n- input: \n\np_JeSfTmbCbLHSvZcU.f_TrzJxHKSvxEaIhMD\ndescription: Generic function, unknown purpose\nargs:\n- available_functions: \n- goal: \n\n\n\n[GOAL]\nmake a receipe on egg receipe and summarize the output \n\n[OUTPUT]\n'}]), chat_prompt_template=None)}), 'ActionPlanner_Excluded': KernelPlugin(name='ActionPlanner_Excluded', description=None, functions={'f_ywbQTmehuZcxpXkJ': KernelFunction(plugin_name='ActionPlanner_Excluded', description='Generic function, unknown purpose', name='f_ywbQTmehuZcxpXkJ', is_semantic=True, stream_function=<function KernelFunction.from_semantic_config.<locals>._local_stream_func at 0x00000170D55BA520>, parameters=[ParameterView(name='input', description='', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.ContextSwitchInKernelContextOutTaskKernelContext: 7>, function=<function KernelFunction.from_semantic_config.<locals>._local_func at 0x00000170D55BA8E0>, plugins=KernelPluginCollection(plugins={...}), ai_service=OpenAIChatCompletion(ai_model_id='gpt-3.5-turbo-1106', client=<openai.AsyncOpenAI object at 0x00000170D4EFC230>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=1654, completion_tokens=462, total_tokens=2116), prompt_execution_settings=OpenAIChatPromptExecutionSettings(service_id=None, extension_data={'max_tokens': 1024, 'stop_sequences': ['#END-OF-PLAN']}, ai_model_id='gpt-3.5-turbo-1106', frequency_penalty=0.0, logit_bias={}, max_tokens=1024, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'A planner takes a list of functions, a goal, and chooses which function to use.\nFor each function the list includes details about the input parameters.\n[START OF EXAMPLES]\n\n[EXAMPLE]\n- List of functions:\n// Read a file.\nFileIOPlugin.ReadAsync\nParameter ""path"": Source file.\n// Write a file.\nFileIOPlugin.WriteAsync\nParameter ""path"": Destination file. (default value: sample.txt)\nParameter ""content"": File content.\n// Get the current time.\nTimePlugin.Time\nNo parameters.\n// Makes a POST request to a uri.\nHttpPlugin.PostAsync\nParameter ""body"": The body of the request.\n- End list of functions.\nGoal: create a file called ""something.txt"".\n{""plan"":{\n""rationale"": ""the list contains a function that allows to create files"",\n""function"": ""FileIOPlugin.WriteAsync"",\n""parameters"": {\n""path"": ""something.txt"",\n""content"": null\n}}}\n#END-OF-PLAN\n\n\n[EXAMPLE]\n- List of functions:\n// Get the current time.\nTimePlugin.Time\nNo parameters.\n// Write a file.\nFileIOPlugin.WriteAsync\nParameter ""path"": Destination file. (default value: sample.txt)\nParameter ""content"": File content.\n// Makes a POST request to a uri.\nHttpPlugin.PostAsync\nParameter ""body"": The body of the request.\n// Read a file.\nFileIOPlugin.ReadAsync\nParameter ""path"": Source file.\n- End list of functions.\nGoal: tell me a joke.\n{""plan"":{\n""rationale"": ""the list does not contain functions to tell jokes or something funny"",\n""function"": """",\n""parameters"": {\n}}}\n#END-OF-PLAN\n\n[END OF EXAMPLES]\n[REAL SCENARIO STARTS HERE]\n- List of functions:\n// provided input generates a receipe for the input.\ncooking.receipegenerator\nParameter ""input"": input.\n// Summarize given text or any text document.\nWriterPlugin.summarize\nParameter ""input"": Text to summarize.\n// Summarize given text in three sentences .\nWriterPlugin.ThreeSentencesummary\nParameter ""input"": input.\n// Generic function, unknown purpose.\np_JeSfTmbCbLHSvZcU.f_TrzJxHKSvxEaIhMD\nParameter ""available_functions"": available_functions.\nParameter ""goal"": goal.\n// Adds value to a value.\nmath.Add\nParameter ""input"": The value to add.\nParameter ""Amount"": Amount to add.\n// Subtracts value to a value.\nmath.Subtract\nParameter ""input"": The value to subtract.\nParameter ""Amount"": Amount to subtract.\n- End list of functions.\nGoal: generate a java program on egg? \n'}]), chat_prompt_template=None), 'EdgeCaseExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few edge case examples of plans to handle', name='EdgeCaseExamples', is_semantic=False, stream_function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x00000170D55C18B0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.edge_case_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x00000170D55C18B0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'GoodExamples': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List a few good examples of plans to generate', name='GoodExamples', is_semantic=False, stream_function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x00000170D55C18B0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.good_examples of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x00000170D55C18B0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None), 'ListOfFunctions': KernelFunction(plugin_name='ActionPlanner_Excluded', description='List all functions available in the kernel', name='ListOfFunctions', is_semantic=False, stream_function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x00000170D55C18B0>>, parameters=[ParameterView(name='goal', description='The current goal processed by the planner', default_value='', type_='string', required=False)], delegate_type=<DelegateTypes.InStringAndContextOutString: 12>, function=<bound method ActionPlanner.list_of_functions of <semantic_kernel.planning.action_planner.action_planner.ActionPlanner object at 0x00000170D55C18B0>>, plugins=KernelPluginCollection(plugins={...}), ai_service=None, prompt_execution_settings=PromptExecutionSettings(service_id=None, extension_data={}), chat_prompt_template=None)})}) ai_service=OpenAIChatCompletion(ai_model_id='gpt-3.5-turbo-1106', client=<openai.AsyncOpenAI object at 0x00000170D4EFC230>, ai_model_type=<OpenAIModelTypes.CHAT: 'chat'>, prompt_tokens=1654, completion_tokens=462, total_tokens=2116) prompt_execution_settings=OpenAIChatPromptExecutionSettings(service_id=None, extension_data={}, ai_model_id='gpt-3.5-turbo-1106', frequency_penalty=0.0, logit_bias={}, max_tokens=256, number_of_responses=1, presence_penalty=0.0, seed=None, stop=None, stream=False, temperature=0.0, top_p=1.0, user=None, response_format=None, tools=None, tool_choice=None, function_call=None, functions=None, messages=[{'role': 'user', 'content': 'Provide a full receipe for egg recipe\n\n'}]) chat_prompt_template=None
INFO:semantic_kernel.planning.action_planner.action_planner:Parameter input: egg
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=214, prompt_tokens=15, total_tokens=229)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=78, prompt_tokens=881, total_tokens=959)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=256, prompt_tokens=15, total_tokens=271)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=67, prompt_tokens=272, total_tokens=339)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=29, prompt_tokens=881, total_tokens=910)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=38, prompt_tokens=881, total_tokens=919)
INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
INFO:semantic_kernel.connectors.ai.open_ai.services.open_ai_handler:OpenAI usage: CompletionUsage(completion_tokens=38, prompt_tokens=881, total_tokens=919)
